+++
widget = "custom"
active = true
date = "2021-05-04T00:00:00"

# Note: a full width section format can be enabled by commenting out the `title` and `subtitle` with a `#`.
title = "Research"
# subtitle = "Leadership in formal education settings"


# Order that this section will appear in.
weight = 30

#One way we could make this different is to present the places of teaching as a smaller list with a click through to course descriptions and why we were teaching at that location. More in line with the project presentation line of reasoning. If we did each teaching engagement independently like a talk, and then aggregate them, then we could use schema.org metadata to describe each teaching engagement.

+++
The research of our lab covers various topics in trustworthy machine learning, such as robustness, privacy and interpretability, as well as their applications in computer vision, natural language processing and cybersecurity. Some of our current projects are:


<h2>Theoretical Fundations of Adversarial ML</h2>

+ [Unerstanding intrinsic robustness via empirically measuring concentration](https://arxiv.org/pdf/1905.12202)
+ [Understanding adversarially robust generalization](https://openreview.net/pdf?id=465Gv50E2N)
+ [Characterizing the optimality of data poisoning attack](https://openreview.net/pdf?id=yyLFUPNEiT)
+ [Limits on membership inference attack](https://arxiv.org/pdf/2406.11544)


<h2> General Aspects of Adversarial ML  </h2>

+ Efficiency of adversarial ML algorithms
+ [Robustness Certification](https://arxiv.org/pdf/2310.08732)
+ Subpopuation data poisoning attacks
+ [Cost-sensitive robustness](https://openreview.net/pdf?id=BygANhA9tQ)


<h2>Adversarial ML Applications</h2>

+ Facial recognition & anti-facial recognition
+ [Image captioning](https://arxiv.org/pdf/2406.05874)
+ Human Pose Estimation
+ Interpretable models


<h2> Generative AI Safety </h2>

+ [Jailbreak attacks & defenses on LLMs](https://arxiv.org/pdf/2403.04783)
+ Prompt injection attacks on LLMs
+ Diffusion models
 